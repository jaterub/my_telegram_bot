{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ff58b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOST: https://dbc-54859cf0-9d64.cloud.databricks.com\n",
      "STATUS: 200 | OK?: True\n",
      "{\"versions\":[{\"key\":\"16.3.x-photon-scala2.12\",\"name\":\"16.3 Photon (includes Apache Spark 3.5.2, Scala 2.12)\"},{\"key\":\"12.2.x-scala2.12\",\"name\":\"12.2 LTS (includes Apache Spark 3.3.2, Scala 2.12)\"},{\"key\":\"11.3.x-photon-scala2.12\",\"name\":\"11.3 LTS Photon (includes Apache Spark 3.3.0, Scala 2.12)\"},{\"key\":\"16.4.x-cpu-ml-scala2.12\",\"name\":\"16.4 LTS ML (includes Apache Spark 3.5.2, Scala 2.12)\"},{\"key\":\"17.2.x-scala2.13\",\"name\":\"17.2 Beta (includes Apache Spark 4.0.0, Scala 2.13)\"},{\"key\":\"17.1.x-cp\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carga .env para esta ejecución\n",
    "load_dotenv()\n",
    "\n",
    "HOST = (os.getenv(\"DATABRICKS_HOST\") or \"\").rstrip(\"/\")\n",
    "TOKEN = os.getenv(\"DATABRICKS_TOKEN\") or \"\"\n",
    "\n",
    "ORG_ID = \"4044162781254195\"  # opcional; si no te funciona, quítalo\n",
    "\n",
    "r = requests.get(\n",
    "    f\"{HOST}/api/2.0/clusters/spark-versions\",\n",
    "    headers={\"Authorization\": f\"Bearer {TOKEN}\", \"X-Databricks-Org-Id\": ORG_ID},\n",
    "    timeout=10,\n",
    ")\n",
    "\n",
    "print(\"HOST:\", HOST)\n",
    "print(\"STATUS:\", r.status_code, \"| OK?:\", r.ok)\n",
    "print(r.text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b317cab",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://dbc-54859cf0-9d64.cloud.databricks.com/api/2.2/jobs/run-now",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m\n\u001b[0;32m     11\u001b[0m b64  \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(csv\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOST\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/2.2/jobs/run-now\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTOKEN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m     16\u001b[0m     json\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: JOBID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebook_params\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv_b64\u001b[39m\u001b[38;5;124m\"\u001b[39m: b64, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmini.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m}},\n\u001b[0;32m     17\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRUN:\u001b[39m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mjson())\n",
      "File \u001b[1;32mc:\\Users\\34680\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://dbc-54859cf0-9d64.cloud.databricks.com/api/2.2/jobs/run-now"
     ]
    }
   ],
   "source": [
    "# scripts/dbx_run_now_b64.py\n",
    "import os, base64, requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "HOST  = (os.getenv(\"DATABRICKS_HOST\") or \"\").rstrip(\"/\")\n",
    "TOKEN = os.getenv(\"DATABRICKS_TOKEN\") or \"\"\n",
    "JOBID = int(os.getenv(\"DATABRICKS_JOB_ID_AUDIT\", \"0\"))\n",
    "\n",
    "csv = \"tx_id,date,account,debit,credit,desc\\n1,2025-01-01,430,100,0,venta\\n1,2025-01-01,700,0,100,venta contrapartida\\n2,2025-13-05,430,50,0,fecha invalida\\n\"\n",
    "b64  = base64.b64encode(csv.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "r = requests.post(\n",
    "    f\"{HOST}/api/2.2/jobs/run-now\",\n",
    "    headers={\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"},\n",
    "    json={\"job_id\": JOBID, \"notebook_params\": {\"csv_b64\": b64, \"file_name\": \"mini.csv\"}},\n",
    "    timeout=30,\n",
    ")\n",
    "r.raise_for_status()\n",
    "print(\"RUN:\", r.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee61fe30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mdbutils\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dbutils' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     dbutils\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdbutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DBUtils\n\u001b[0;32m     10\u001b[0m     dbutils \u001b[38;5;241m=\u001b[39m DBUtils(spark)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbase64\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook: bot_audit_xlsx_b64 (solo Excel, versión robusta)\n",
    "# Añadido: validación regex para fechas -> evita errores\n",
    "# ============================================================\n",
    "\n",
    "try:\n",
    "    dbutils\n",
    "except NameError:\n",
    "    from pyspark.dbutils import DBUtils\n",
    "    dbutils = DBUtils(spark)\n",
    "\n",
    "import base64, io, json\n",
    "import pandas as pd\n",
    "from pyspark.sql import types as T, functions as F\n",
    "\n",
    "# --- DEMO Excel si no hay file_b64 -------------------------------------------\n",
    "file_b64 = (dbutils.widgets.get(\"file_b64\") or \"\").strip()\n",
    "if not file_b64:\n",
    "    demo_df = pd.DataFrame({\n",
    "        \"tx_id\":   [\"1\",\"1\",\"2\",\"3\"],\n",
    "        \"date\":    [\"2025-01-02\",\"2025-01-02\",\"2025-13-05\",\"2025-02-10\"],\n",
    "        \"account\": [\"430\",\"700\",\"430\",\"430\"],\n",
    "        \"debit\":   [100, 0, 50, 20],\n",
    "        \"credit\":  [0, 100, 0, 0],\n",
    "        \"desc\":    [\"Venta A\",\"Venta A contrapartida\",\"Fecha inválida\",\"Desbalance\"],\n",
    "    })\n",
    "    buf = io.BytesIO()\n",
    "    with pd.ExcelWriter(buf, engine=\"xlsxwriter\") as writer:\n",
    "        demo_df.to_excel(writer, index=False, sheet_name=\"Sheet1\")\n",
    "    file_b64 = base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "raw = base64.b64decode(file_b64.encode(\"utf-8\"))\n",
    "pdf = pd.read_excel(io.BytesIO(raw), sheet_name=0, dtype=str)\n",
    "\n",
    "# --- Normalización -----------------------------------------------------------\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"tx_id\",   T.StringType(), True),\n",
    "    T.StructField(\"date\",    T.StringType(), True),\n",
    "    T.StructField(\"account\", T.StringType(), True),\n",
    "    T.StructField(\"debit\",   T.DoubleType(), True),\n",
    "    T.StructField(\"credit\",  T.DoubleType(), True),\n",
    "    T.StructField(\"desc\",    T.StringType(), True),\n",
    "])\n",
    "df = spark.createDataFrame(pdf, schema=schema)\n",
    "\n",
    "# --- Validación de fechas ----------------------------------------------------\n",
    "df1 = df.withColumn(\n",
    "    \"is_valid_pattern\",\n",
    "    F.col(\"date\").rlike(r\"^\\d{4}-(0[1-9]|1[0-2])-(0[1-9]|[12]\\d|3[01])$\")\n",
    ")\n",
    "# Solo parseamos si cumple el patrón\n",
    "df1 = df1.withColumn(\n",
    "    \"date_parsed\",\n",
    "    F.when(F.col(\"is_valid_pattern\"), F.to_date(\"date\", \"yyyy-MM-dd\"))\n",
    ")\n",
    "\n",
    "# --- Auditoría ---------------------------------------------------------------\n",
    "invalid_date   = df1.filter(~F.col(\"is_valid_pattern\") & F.col(\"date\").isNotNull())\n",
    "dups           = df1.groupBy(\"tx_id\").count().filter(F.col(\"count\") > 1)\n",
    "agg            = df1.groupBy(\"tx_id\").agg(F.sum(\"debit\").alias(\"sum_debit\"), F.sum(\"credit\").alias(\"sum_credit\"))\n",
    "unbalanced     = agg.withColumn(\"diff\", F.col(\"sum_debit\") - F.col(\"sum_credit\")).filter(F.abs(F.col(\"diff\")) > 1e-6)\n",
    "required_nulls = df1.filter(F.col(\"tx_id\").isNull() | F.col(\"account\").isNull() | F.col(\"date\").isNull())\n",
    "\n",
    "summary = {\n",
    "    \"rows\":            df1.count(),\n",
    "    \"invalid_date\":    {\"count\": invalid_date.count()},\n",
    "    \"duplicates_tx\":   {\"count\": dups.count()},\n",
    "    \"unbalanced_tx\":   {\"count\": unbalanced.count()},\n",
    "    \"required_nulls\":  {\"count\": required_nulls.count()},\n",
    "}\n",
    "\n",
    "# --- Salida ------------------------------------------------------------------\n",
    "result = json.dumps(summary, ensure_ascii=False, indent=2)\n",
    "try:\n",
    "    dbutils.notebook.exit(result)   # si lo corres como Job\n",
    "except Exception:\n",
    "    print(result)                   # si lo corres a mano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d342e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace status: 200 {\"object_type\":\"DIRECTORY\",\"path\":\"/\",\"object_id\":0,\"resource_id\":\"0\"}\n",
      "\n",
      "Jobs.get: 200 {\"job_id\":949482580389040,\"creator_user_name\":\"ja.tejeror@gmail.com\",\"run_as_user_name\":\"ja.tejeror@gmail.com\",\"run_as_owner\":true,\"settings\":{\"name\":\"New Job Sep 07, 2025, 03:43 AM\",\"email_notificati\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "host   = (os.getenv(\"DATABRICKS_HOST\") or \"\").rstrip(\"/\")\n",
    "token  = os.getenv(\"DATABRICKS_TOKEN\")\n",
    "job_id = os.getenv(\"DATABRICKS_JOB_ID_AUDIT\")\n",
    "\n",
    "assert host and token and job_id, \"Faltan vars\"\n",
    "h = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "# 1) ¿Responde el workspace?\n",
    "r = requests.get(host + \"/api/2.0/workspace/get-status\", headers=h, params={\"path\":\"/\"}, timeout=30)\n",
    "print(\"Workspace status:\", r.status_code, r.text[:120])\n",
    "\n",
    "# 2) ¿Existe el Job?\n",
    "r = requests.get(host + \"/api/2.2/jobs/get\", headers=h, params={\"job_id\": job_id}, timeout=30)\n",
    "print(\"Jobs.get:\", r.status_code, r.text[:200])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4bca994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59a9078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Crear el DataFrame\n",
    "demo = pd.DataFrame({\n",
    "    \"tx_id\":   [\"1\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"9\",\"9\",\"9\"],\n",
    "    \"date\":    [\"2025-01-02\",\"2025-01-02\",\"2025-13-05\",\"2025-02-10\",\"\", \"2025-02-11\",\"2025-02-11\",\"2025-03-03\",\"2025-03-03\",\"2025-03-03\"],\n",
    "    \"account\": [\"430\",\"700\",\"430\",\"430\",\"430\",\"430\",\"430\",\"430\",\"700\",\"570\"],\n",
    "    \"debit\":   [\"100\", \"0\", \"50\", \"20\", \"10\", \"€ 1.234,56\", \"10\", \"100\", \"0\", \"0\"],\n",
    "    \"credit\":  [\"0\", \"100\", \"0\", \"0\", \"10\", \"10\", \"—\", \"0\", \"100\", \"10\"],\n",
    "    \"desc\":    [\"Venta A\",\"Venta A contrapartida\",\"Fecha inválida\",\"Desbalance\",\"Falta fecha\",\"Debit nulo\",\"Credit nulo\",\"Dup A\",\"Dup B balanceado\",\"Extra línea desbalancea\"]\n",
    "})\n",
    "\n",
    "demo.to_excel(\"audit_demo.xlsx\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
